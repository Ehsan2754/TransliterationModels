{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ehsan Shaghaei\n",
    "# B19-AAI01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Imports, Defines and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Importing the neccessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "from functools import reduce\n",
    "import gc\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data path\n",
    "data_paths = {\n",
    "    # 'distro':'./tag_logit_per_word.tsv',\n",
    "    'train': './train_pos.txt',\n",
    "    'test': './test_pos.txt',\n",
    "}\n",
    "random_data = pd.read_csv('./tag_logit_per_word.tsv', sep='\\t')\n",
    "random_data.set_index(random_data.columns[0],inplace=True)\n",
    "# display(random_data)\n",
    "random_data= random_data.to_dict()\n",
    "# Funtion to load the raw data\n",
    "\n",
    "\n",
    "def load_data(data_paths: dict):\n",
    "    # Allowing Blank Lines since we can find the end of sentences\n",
    "    return {key: pd.read_csv(path, sep=' ', skip_blank_lines=False, header=None) for key, path in data_paths.items()}\n",
    "\n",
    "\n",
    "# Loading the data\n",
    "data = load_data(data_paths)\n",
    "\n",
    "# Finding the indices of the end of the sentences locating the \"Blank Lines\"\n",
    "endOfSentenceIndexes = dict(map(\n",
    "    lambda x:\n",
    "        (x[0], x[1][x[1].isna().any(axis=1)].index),\n",
    "    data.items()))\n",
    "\n",
    "\n",
    "# Extracting sentences for train and test data\n",
    "for key, df in data.items():\n",
    "    df.rename(columns={0: 'token', 1: 'tag'}, inplace=True)\n",
    "\n",
    "prev_ind = 0\n",
    "train_sentences = []\n",
    "for current_ind in endOfSentenceIndexes['train']:\n",
    "    sentence = data['train'][prev_ind:current_ind:]\n",
    "    sentence.reset_index(drop=True, inplace=True)\n",
    "    train_sentences.append(sentence)\n",
    "    prev_ind = current_ind+1\n",
    "\n",
    "prev_ind = 0\n",
    "test_sentences = []\n",
    "for current_ind in endOfSentenceIndexes['test']:\n",
    "    sentence = data['test'][prev_ind:current_ind:]\n",
    "    sentence.reset_index(drop=True, inplace=True)\n",
    "    test_sentences.append(sentence)\n",
    "    prev_ind = current_ind+1\n",
    "\n",
    "# Creating the Dictionary for a faster access\n",
    "train_list, test_list = [], []\n",
    "# print('Loading Train data ... ')\n",
    "for df in (train_sentences):\n",
    "    train_list.append({row[1].token: row[1].tag for row in df.iterrows()})\n",
    "# print('Loading Test data ... ')\n",
    "for df in (test_sentences):\n",
    "    test_list.append({row[1].token: row[1].tag for row in df.iterrows()})\n",
    "\n",
    "# Assigning the data\n",
    "data['train'] = train_list\n",
    "data['test'] = test_list\n",
    "\n",
    "# Garbage collection and preliminary evaluation\n",
    "gc.collect()\n",
    "# display('First Train Sentence',data['train'][0])\n",
    "# display('First Test Sentence',data['test'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Calculate the transition probability and emission matrices (First step towards viterbi) - 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding all unique tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set()\n",
    "# print('Finding unique tags')\n",
    "for sentence in (data['train']):\n",
    "    unique_tags = unique_tags.union(set(sentence.values()))\n",
    "\n",
    "# pd.DataFrame(unique_tags,columns=['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding all unique tokens\\[words\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = set()\n",
    "# print('Finding uique tokens')\n",
    "for sentence in (data['train']):\n",
    "    unique_tokens = unique_tokens.union(set(sentence.keys()))\n",
    "\n",
    "# pd.DataFrame(unique_tokens,columns=['token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probability Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing Transition Count Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing factor for TPM\n",
    "alpha = 0.001\n",
    "n_tags = len(unique_tags)\n",
    "\n",
    "# Start tag\n",
    "startPOS = 'START'\n",
    "transitionProbabilityMatrixDict = {start_tag:{end_tag:0.0 for end_tag in unique_tags} for start_tag in [startPOS]+list(unique_tags) }\n",
    "\n",
    "# Counting the transitions\n",
    "# print('Counting Transitions')\n",
    "for dict in (data['train']):\n",
    "    current_tag=startPOS\n",
    "    for next_tag in dict.values():\n",
    "        transitionProbabilityMatrixDict[current_tag][next_tag]+=1\n",
    "        current_tag = next_tag\n",
    "\n",
    "transitionProbabilityMatrix = pd.DataFrame.from_dict(transitionProbabilityMatrixDict)+alpha\n",
    "# display(transitionProbabilityMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary test of the count results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transitionsFromSTART2x(x, data):\n",
    "    return len(re.findall(f'\\n \\n\\w+ {x}', data))\n",
    "\n",
    "\n",
    "with open(data_paths['train'], 'r') as f:\n",
    "    t = f.read()\n",
    "    assert((transitionsFromSTART2x('VBG', t)) ==\n",
    "           transitionProbabilityMatrixDict[startPOS]['VBG']), \"in Transition Probability Matrix, Transition count 'START->VBG' is INVALID\"\n",
    "    assert((transitionsFromSTART2x('VBN', t)) ==\n",
    "           transitionProbabilityMatrixDict[startPOS]['VBN']), \"in Transition Probability Matrix, Transition count 'START->VBN' s INVALID\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing and smoothing the transition vectors to obtain probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Normalizing and smoothing TPM')\n",
    "for start_tag in (transitionProbabilityMatrix.columns):\n",
    "    transitionProbabilityMatrix[start_tag] /= (\n",
    "        np.sum(transitionProbabilityMatrix[start_tag]) + n_tags*alpha)\n",
    "transitionProbabilityMatrixDict = transitionProbabilityMatrix.to_dict()\n",
    "# transitionProbabilityMatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Transition Probability Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(transitionProbabilityMatrix < 1) and all(transitionProbabilityMatrix.all() >=0) , ' The Transition Probability Matrix has invalid probability value'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothening Transition Probability Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emission Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing Emiss\n",
    "ion Count Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing factor for TPM\n",
    "beta = 0.01\n",
    "n_tags = len(unique_tags)\n",
    "\n",
    "emissionMatrixDict = {token:{tag:0.0 for tag in unique_tags} for token in unique_tokens}\n",
    "# print('Counting Emissions')\n",
    "# Counting the transitions\n",
    "for dict in (data['train']):\n",
    "    for token,tag in dict.items():\n",
    "        emissionMatrixDict[token][tag]+=1\n",
    "\n",
    "emissionMatrix = pd.DataFrame.from_dict(emissionMatrixDict)+beta\n",
    "# display(emissionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary test of the count results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emissionCNTbyRegex(token,tag,data):\n",
    "    return len(re.findall(f'\\n{token} {tag}\\n',data))\n",
    "\n",
    "with open(data_paths['train'],'r') as f:\n",
    "    t = f.read()\n",
    "    assert(emissionCNTbyRegex('want','VB',t)==emissionMatrixDict['want']['VB']),\"in Emition Matrix, Emition count 'want -> VB' is INVALID\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing and smoothing the emission vectors to obtain probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Normalizing and smoothening the EPM')\n",
    "for token in (emissionMatrix.columns):\n",
    "    emissionMatrix[token]/=(np.sum(emissionMatrix[token])+(beta*n_tags))\n",
    "emissionMatrixDict = emissionMatrix.to_dict()\n",
    "# emissionMatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Emission Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert all(emissionMatrix < 1) and all(emissionMatrix.all() >=0) , ' The Emittion Matrix has invalid probability value'\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Implement Viterbi algorithm for POS tagging task. - 30 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of so called best_probabilities and best_paths matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tpm_T = transitionProbabilityMatrix.to_dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getVector(d):\n",
    "    return np.array(list(d.values()))\n",
    "\n",
    "# feed-forward of best_probability matrix C_ij\n",
    "def feedforward(i, j,bpr,token_vector,tags_vector):\n",
    "    # C[k][j-1] X A[K][i] X B[i][j]\n",
    "    dest_vector = getVector(bpr[token_vector[j-1]]) +\\\n",
    "        np.log(getVector(tpm_T[tags_vector[i]])) +  \\\n",
    "        np.log(random_data[tags_vector[i]][token_vector[j]]) if not token in unique_tokens else getVector(bpr[token_vector[j-1]]) +\\\n",
    "        np.log(getVector(tpm_T[tags_vector[i]])) +  \\\n",
    "        np.log(emissionMatrixDict[token_vector[j]][tags_vector[i]])\n",
    "    # return best_probability and best_path\n",
    "    return np.max(dest_vector), np.argmax(dest_vector)\n",
    "\n",
    "\n",
    "\n",
    "def generate_best_probability_path_matrix(tokens):\n",
    "    tokens_vector = list(tokens)\n",
    "    tags_vector = list(unique_tags)\n",
    "    bpr, bpa = {token: {tag: 0.0 for tag in tags_vector}\n",
    "                for token in tokens_vector}, {token: {tag: 0 for tag in tags_vector}\n",
    "                                       for token in tokens_vector}\n",
    "\n",
    "    # print('Initialization of the first column of best_probability')\n",
    "    for token in bpr.keys():\n",
    "        for tag in bpr[token].keys():\n",
    "            if token in unique_tokens:\n",
    "                bpr[token][tag] = np.log(transitionProbabilityMatrixDict[startPOS][tag])+np.log(emissionMatrixDict[token][tag])\n",
    "            else:\n",
    "                bpr[token][tag] = np.log(transitionProbabilityMatrixDict[startPOS][tag])+np.log(random_data[tag][token])\n",
    "        # display(pd.DataFrame.from_dict(bpr))\n",
    "        break  # --! First column only for now\n",
    "\n",
    "    # print('Initialization of the first column of best_path')\n",
    "    for token in (bpa.keys()):\n",
    "        for tag in bpa[token].keys():\n",
    "            bpa[token][tag] = 0\n",
    "        # display(pd.DataFrame.from_dict(bpa))\n",
    "        break  # --! First column only for now\n",
    "\n",
    "    # print('feeding-forward')\n",
    "    # skipping the initialized column\n",
    "    for j, token in (enumerate(tokens_vector[1:], 1)):\n",
    "        for i, tag in enumerate(tags_vector):\n",
    "            bpr[token][tag], bpa[token][tag] = feedforward(\n",
    "                i, j,bpr,tokens_vector,tags_vector)\n",
    "    pred_tags=[]\n",
    "    # Feed-Backward\n",
    "    for j, token in reversed(list(enumerate(tokens_vector))):\n",
    "        next_tag_id=0\n",
    "        if j == len(tokens_vector)-1:\n",
    "            next_tag_id = np.argmax(\n",
    "                getVector(bpr[token])\n",
    "            )\n",
    "        else:\n",
    "            next_tag_id = bpa[token][tags_vector[next_tag_id]]\n",
    "        pred_tags.append(tags_vector[next_tag_id])\n",
    "    pred_tags = list(reversed(pred_tags))\n",
    "        \n",
    "    \n",
    "\n",
    "    return pred_tags,bpr,bpa     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Test your viterbi algorithm on the test set and record the accuracy. The accuray referes to the number of correcly predicted tags in the whole test samples. - 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaingig test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahuratus\\AppData\\Local\\Temp/ipykernel_23580/438233198.py:36: RuntimeWarning: invalid value encountered in log\n",
      "  bpr[token][tag] = np.log(transitionProbabilityMatrixDict[startPOS][tag])+np.log(random_data[tag][token])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'extending'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23580/1837269650.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtruth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# print(all_tokens)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_best_probability_path_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mtruth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23580/438233198.py\u001b[0m in \u001b[0;36mgenerate_best_probability_path_matrix\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             bpr[token][tag], bpa[token][tag] = feedforward(\n\u001b[0m\u001b[0;32m     52\u001b[0m                 i, j,bpr,tokens_vector,tags_vector)\n\u001b[0;32m     53\u001b[0m     \u001b[0mpred_tags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23580/438233198.py\u001b[0m in \u001b[0;36mfeedforward\u001b[1;34m(i, j, bpr, token_vector, tags_vector)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtags_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_tokens\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mgetVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpm_T\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtags_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memissionMatrixDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtags_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m# return best_probability and best_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdest_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdest_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'extending'"
     ]
    }
   ],
   "source": [
    "test = data['test']\n",
    "y = []\n",
    "y_pred = []\n",
    "for sentence in test:\n",
    "    # print(sentence)\n",
    "    tokens = list(sentence.keys())\n",
    "    truth = list(sentence.values())\n",
    "    # print(all_tokens)\n",
    "    pred,_,_ = generate_best_probability_path_matrix(tokens)\n",
    "    y+=truth\n",
    "    y_pred+=pred\n",
    "\n",
    "\n",
    "\n",
    "def calc_accuracy(y,y_pred):\n",
    "    return np.sum(np.array(y)==np.array(y_pred))/len(y)\n",
    "print(calc_accuracy(y,y_pred))\n",
    "# # print()\n",
    "# p,q,r= generate_best_probability_path_matrix(test[1].keys())\n",
    "# np.sum(np.array(p)==np.array(test[1].values()))\n",
    "# np.sum(np.array([1,1,1])==np.array([1,2,1]))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad69a6eeded1810137902dabe12f7736dcdf84919ba34f8c521f07deeeecf2ea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
